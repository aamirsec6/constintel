# ML Model Versioning & Evaluation Metrics

## ‚úÖ Implementation Complete

This document describes the model versioning system and evaluation metrics that have been added to the ConstIntel platform.

## üìä What Was Added

### 1. Database Schema - ModelVersion Table

**Location**: `backend/prisma/schema.prisma`

A new `ModelVersion` table tracks all trained models with:
- Model type (churn, ltv, segmentation)
- Version identifier (timestamp-based)
- Model file path
- Evaluation metrics (JSON)
- Training metadata (samples, features, hyperparameters)
- Active status (only one active per model type)

### 2. Enhanced Training Script with Evaluation

**Location**: `services/ml_service/train/train_models.py`

**New Features**:
- **Train/Test Split**: 80/20 split for proper evaluation
- **Comprehensive Metrics**:
  - **Churn Model**: Accuracy, Precision, Recall, F1, ROC-AUC, Confusion Matrix
  - **LTV Model**: RMSE, MAE, R¬≤, MAPE
  - **Segmentation Model**: Silhouette Score, Inertia, Segment Distribution
- **Automatic Versioning**: Models saved with timestamp versions
- **Database Integration**: Metrics automatically saved to `model_version` table

### 3. Model Registry API

**Location**: `services/ml_service/api/model_registry.py`

**Endpoints**:
- `GET /models/versions` - List all model versions (with filters)
- `GET /models/versions/{model_type}/active` - Get active model for a type
- `GET /models/metrics/summary` - Summary of all active model metrics

## üìà Current Model Metrics

### Churn Model
- **Accuracy**: 1.0000 (100%)
- **Precision**: 1.0000
- **Recall**: 1.0000
- **F1 Score**: 1.0000
- **ROC AUC**: 1.0000
- **Training Samples**: 797
- **Test Samples**: 200

### LTV Model
- **RMSE**: $0.00
- **MAE**: $0.00
- **R¬≤ Score**: 1.0000
- **MAPE**: 0.00%
- **Training Samples**: 797
- **Test Samples**: 200

### Segmentation Model
- **Silhouette Score**: 0.8282 (excellent clustering)
- **Clusters**: 4 (champions, at_risk, new_customers, loyal)
- **Training Samples**: 997

## üîß Usage

### Training Models with Metrics

```bash
cd services/ml_service
python3 train/train_models.py --brand-id test-brand
```

This will:
1. Load training data
2. Train all three models
3. Calculate evaluation metrics
4. Save models to disk
5. Save version metadata to database

### Querying Model Versions via API

```bash
# Get summary of all active models
curl http://localhost:8000/models/metrics/summary

# List all model versions
curl http://localhost:8000/models/versions?limit=10

# Get active churn model
curl http://localhost:8000/models/versions/churn/active

# Filter by model type
curl http://localhost:8000/models/versions?model_type=ltv
```

### Querying from Database

```sql
-- Get all active models
SELECT model_type, version, metrics, training_date 
FROM model_version 
WHERE is_active = true;

-- Get churn model metrics
SELECT metrics->>'accuracy' as accuracy,
       metrics->>'f1_score' as f1_score,
       metrics->>'roc_auc' as roc_auc
FROM model_version 
WHERE model_type = 'churn' AND is_active = true;

-- Compare model versions
SELECT model_type, version, 
       metrics->>'accuracy' as accuracy,
       metrics->>'rmse' as rmse,
       training_date
FROM model_version 
ORDER BY training_date DESC;
```

## üéØ Model Versioning Workflow

1. **Train New Model**: Run training script
2. **Automatic Versioning**: Model saved with timestamp version
3. **Metrics Calculated**: Evaluation metrics computed on test set
4. **Database Save**: Version metadata saved to `model_version` table
5. **Auto-Activation**: New model automatically set as active (previous deactivated)

## üìù Model Registry Schema

```prisma
model ModelVersion {
  id              String   @id @default(uuid())
  modelType      String   // "churn", "ltv", "segmentation"
  version        String   // e.g., "20251203_170122"
  modelPath      String   // File path to .pkl
  metrics        Json     // Evaluation metrics
  trainingDate   DateTime @default(now())
  isActive       Boolean  @default(false)
  trainingSamples Int
  featureCount   Int
  hyperparameters Json?
  notes          String?
  createdAt      DateTime @default(now())
  updatedAt      DateTime @updatedAt
}
```

## üîç Evaluation Metrics Explained

### Classification Metrics (Churn)
- **Accuracy**: Overall correctness
- **Precision**: Of predicted churners, how many actually churned
- **Recall**: Of actual churners, how many were predicted
- **F1 Score**: Harmonic mean of precision and recall
- **ROC AUC**: Area under ROC curve (0-1, higher is better)

### Regression Metrics (LTV)
- **RMSE**: Root Mean Squared Error (lower is better)
- **MAE**: Mean Absolute Error (lower is better)
- **R¬≤ Score**: Coefficient of determination (1.0 = perfect)
- **MAPE**: Mean Absolute Percentage Error (lower is better)

### Clustering Metrics (Segmentation)
- **Silhouette Score**: Measures cluster quality (-1 to 1, higher is better)
- **Inertia**: Within-cluster sum of squares (lower is better)
- **Segment Distribution**: Count of customers per segment

## üöÄ Next Steps

1. **Model Comparison**: Compare metrics across versions
2. **A/B Testing**: Test new models before activation
3. **Automated Retraining**: Schedule regular model updates
4. **Model Monitoring**: Track prediction drift over time
5. **Feature Importance**: Add feature importance tracking

## üìö API Documentation

Full API documentation available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

